{
  "clicked": false,
  "_replaced_more": false,
  "archived": false,
  "post_hint": "self",
  "num_comments": 40,
  "created": 1442742131.0,
  "saved": false,
  "selftext": "New [Proof](http://www.3dmark.com/3dm/8633319?) & [Demo](https://www.youtube.com/watch?v=5bfUF9V7Uto). 14.5k Firestrike score on a 980ti in a VM when 15k is the norm. Roughly a 3.33% loss in performance. \n\nHey everyone. I made a guide on how to do GPU passthrough 6 months ago located [here](https://www.reddit.com/r/pcmasterrace/comments/2z0evz/gpu_passthrough_or_how_to_play_any_game_at_near/) and figured it was time to revisit it. It's currently the 2nd result on google when you search for \"gpu passthrough\" and I still get messages quite frequently about it so for the sake of my sanity and others looking to do this I'll go ahead and give everyone a rundown of the best way to achieve this with current methods. This is meant to be a start to finish, **holy shit this actually works**, guide and is another lengthy post because there's a lot to cover so stick with it and you'll be happy you did. This post is going to cover UEFI specific hardware because every GPU made in the last few years has had it. I believe Nvidia implemented UEFI bios in the 600 series cards and some of you may need to flash the bios for that support, so all of you 500 series owners looking to pass them through will need to refer to the previous post and you want the q35 bios method. I'm unsure on when exactly AMD implemented support so do your research, I'm also supporting Intel/Nvidia exclusively in this post because I don't own any AMD hardware. Everyone else can continue reading. \n\n------------------------\n**My Hardware Config** \n\nCPU: i7 4790k\n\nFor those that skipped the first tutorial you need VT-d. This is our bread and butter tech that allows us to pass through the GPU. \n\nMobo: MSI z97a Gaming 7 \n\nIt's a bit of an upgrade for me because my g1.sniper h6 was giving me fits when I upgraded to 32GB of ram. Manual doesn't say the MSI supports VT-d specifically but does say it supports \"virtualization technology\" which is what we're after. \n\nGPU: 1 x EVGA GTX 970 FTW, 1 x ASUS STRIX GTX 980ti\n\nI got a fancy 1440p 144hz monitor and the 970 wasn't cutting it so I picked up a 980ti. You ***do not*** need these expensive cards to achieve this. The 980ti supports UEFI. That's all we're after in this post. I've done this with everything from a gtx 260, 550ti, 970, and now 980ti. \n\nStorage: 1 x Intel 730 240GB SSD, 4 x 1TB WD in Raid 10\n\nI changed my storage up a bit for peace of mind reasons. I'm now running windows off of a qcow2 container rather than a physical hdd. You can do either! If you would like to run it off a physical disk refer to the previous post. \n\nRAM: 32GB of ddr3 1600\n\nLinux doesn't require much ram. I simply have 32GB because I run a lot of VM's for work and emulate an enterprise environment. Here's my current usage. You're free to allocate however much you want to windows or linux.\n\n\n    total     used\n    32126     1399\n\nMonitors: 2 x 1920x1080, 1 x 1440p \n\nThe gist of monitor setups with this is to wire everything twice. You need one cable going out of each GPU to each monitor. If your monitor only has one input you can buy a switch for whatever connection you need. If you get a switch you can either leave both outputs enabled all the time or turn the linux output off with xrandr and the switch should failover automatically to the second input. If you don't have a switch then I would recommend using xrandr because it gets annoying to manually switch the inputs every time on the monitor. \n\n--------------------------\n\n**Setup and Installation**\n\nI'm lazy as shit and break things constantly. So I'm using Antergos to install Arch. I put my /home on the raid10 and just change /etc/fstab if I need to reinstall so I never lose anything important. Pretty much any offshoot of Arch and vanilla Arch should work just fine for this post. I have also helped people do this on Debian and derivatives. For this guide we will be sticking to Arch. I'm using grub2 as my bootloader with UEFI.  \n\nThe first thing we need to do is enable VT-d and ensure functionality. We need to edit /etc/default/grub . Look for the line that says \"GRUB_CMDLINE_DEFAULT=\"\" and append \"intel_iommu=on\" to what is inside of that line. Mine looks like this\n\n    GRUB_CMDLINE_LINUX_DEFAULT=\"resume=UUID=6771936b-06b6-493c-b655-6f60122f5228 intel_iommu=on\"\n    \nOnce you've done that we need to rebuild our grub.cfg file. Run this to do so\n\n    sudo grub-mkconfig -o /boot/grub/grub.cfg\n\nNext we reboot to activate iommu/vt-d. Once you're back in Arch we need to verify that VT-d is enabled and functioning. First we need to identify the pci-e bus the GPU we're passing through is on. We run \"lspci -nnk\" to find this information. Here are the lines important to me.  \n\n    02:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] [10de:17c8] (rev a1)\n    \tSubsystem: ASUSTeK Computer Inc. Device [1043:8548]\n    \tKernel driver in use: nvidia\n    \tKernel modules: nouveau, nvidia\n    02:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:0fb0] (rev a1)\n    \tSubsystem: ASUSTeK Computer Inc. Device [1043:8548]\n    \tKernel driver in use: snd_hda_in\n\nMy 980ti is in the second pci-e slot on my motherboard so this is correct. 02.00.1 is the audio bus for the card and is also important later on. Next we need to see if the 980ti falls into another pci-e bus's iommu group and if that will conflict. To find this out you look in /sys/bus/pci/devices/YOUR_BUS/iommu_group/devices/. If you do not have an iommu_group folder then vt-d was not enabled properly! Here is the output for mine\n\n    [kemmler@arch ~]$ ls -lha /sys/bus/pci/devices/0000\\:02\\:00.0/iommu_group/devices/\n    total 0\n    drwxr-xr-x 2 root root 0 Sep 19 18:47 .\n    drwxr-xr-x 3 root root 0 Sep 19 18:46 ..\n    lrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:00:01.0 -> ../../../../devices/pci0000:00/0000:00:01.0\n    lrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:00:01.1 -> ../../../../devices/pci0000:00/0000:00:01.1\n    lrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:01:00.0 -> ../../../../devices/pci0000:00/0000:00:01.0/0000:01:00.0\n    lrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:01:00.1 -> ../../../../devices/pci0000:00/0000:00:01.0/0000:01:00.1\n    lrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:02:00.0 -> ../../../../devices/pci0000:00/0000:00:01.1/0000:02:00.0\n    lrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:02:00.1 -> ../../../../devices/pci0000:00/0000:00:01.1/0000:02:00.1\n\n\n0000:00:01.0 and 0000:00:01.1 can be ignored. The issue is 0000:01:00.0 and 0000:01:00.1. These are my 970 and will cause GPU passthrough to fail unless I pass both cards through to the VM. If you only see device of 0000:0#.00.# in your output then your iommu group is clean and you can skip this next section\n\n----------------------------------------\n\n**Fixing IOMMU Grouping**\n\nI installed Antergos with AUR support. This provides a tool called yaourt. A very thoughtful member of the Arch community has been steadily providing a current kernel patched with a few fixes that deal with IOMMU groups and other issues. This package is called \"linux-vfio\". Assuming you have yaourt installed you will run this. \n\n    yaourt -S linux-vfio\n\nIt will ask you if you want to edit a few things, you can say no. It will then ask you if you want to only install linux-vfio. Say **NO** so it will also install the docs and headers for the kernel. Proceed through the installation with common sense. Once you've installed it, rebuild your grub.cfg as above with \"sudo grub-mkconfig -o /boot/grub/grub.cfg\". \n\n///\n\nIf you installed the \"nvidia\" package from the arch repo your driver will probably break in the new kernel. I simply installed the binary from nvidia's website. A simple way to do that against the new kernel is to download the binary, reboot, edit grub by pressing \"e\" with linux-vfio selected, and then append \"nomodeset systemd.unit=multi-user.target\" to the end of the long line that begins with \"linux ...\" so you do a one time edit of the boot parameters. Then navigate to the binary and \"sh NVIDIA-###...sh\" It should disable nouveau if needed and install. Reboot and continue. You may also want to look into nvidia-dkms if you plan on updating your linux-vfio kernel regularly. \n\n///\n\nOnce you're in the linux-vfio kernel you need to enable the acs_override patch. The easiest way is to just use the downstream method. There are other [options](https://lkml.org/lkml/2013/5/30/513) but should not be necessary. We will add \"pcie_acs_override=downstream\" to our grub.cfg at /etc/default/grub. \"sudo grub-mkconfig -o /boot/grub/grub.cfg\" once again to rebuild it. \n\n    GRUB_CMDLINE_LINUX_DEFAULT=\"resume=UUID=6771936b-06b6-493c-b655-6f60122f5228 pcie_acs_override=downstream intel_iommu=on\"\n    \nReboot and then check your iommu group again. \n\n    [kemmler@arch ~]$ ls -lha /sys/bus/pci/devices/0000\\:02\\:00.0/iommu_group/devices/\n    total 0\n    drwxr-xr-x 2 root root 0 Sep 19 19:11 .\n    drwxr-xr-x 3 root root 0 Sep 19 19:11 ..\n    lrwxrwxrwx 1 root root 0 Sep 19 19:11 0000:02:00.0 -> ../../../../devices/pci0000:00/0000:00:01.1/0000:02:00.0\n    lrwxrwxrwx 1 root root 0 Sep 19 19:11 0000:02:00.1 -> ../../../../devices/pci0000:00/0000:00:01.1/0000:02:00.1\n\n0000:01:00.0/1 are now missing from the initial output. This is exactly what we want to see. This means that the 980ti is now in it's own IOMMU group and can be allocated to a VM by itself. We're now ready to move on. \n\n--------------------------------------\n\n**Setup Continued**\n\nThe next thing we need to do is blacklist the GPU we're passing through to the VM so that the Nvidia driver doesn't try to grab it. Nvidia is a dick and doesn't conform to standards properly. You can't easily unbind a gpu from the Nvidia driver so we use a module called \"pci-stub\" to claim the card before nvidia can. We achieve this by putting pci-stub in our initramfs that is loaded before the kernel and passing a parameter to grub telling it what to do. So, edit \"/etc/mkinitcpio.conf\" and add \"pci-stub\" to the Modules=\"\" section like so \n\n    MODULES=\"pci-stub\"\n    \nIf you're running the stock kernel use this to rebuild your initramfs\n\n    sudo mkinitcpio -p linux\n\nlinux-vfio users run this\n\n    sudo mkinitcpio -p linux-vfio\n\nNow we edit grub once again and add our pci_stub options to bind our card to pci-stub. Get your device IDs from \"lspci -nnk\". My id's are \"10de:17c8\" and \"10de:0fb0\" as seen here again\n\n    02:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] [10de:17c8] (rev a1)\n    \tSubsystem: ASUSTeK Computer Inc. Device [1043:8548]\n    \tKernel driver in use: nvidia\n    \tKernel modules: nouveau, nvidia\n    02:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:0fb0] (rev a1)\n    \tSubsystem: ASUSTeK Computer Inc. Device [1043:8548]\n    \tKernel driver in use: snd_hda_intel\n    \tKernel modules: snd_hda_intel\n\nEdit /etc/default/grub and then run \"sudo grub-mkconfig -o /boot/grub/grub.cfg\" Your line should similar to this. Remember that you need to pass what's in the IOMMU group so you need the main card and the audio bus if it's there. \n\n    GRUB_CMDLINE_LINUX_DEFAULT=\"resume=UUID=6771936b-06b6-493c-b655-6f60122f5228 pcie_acs_override=downstream intel_iommu=on pci-stub.ids=10de:17c8,10de:0fb0\"\n    \nReboot. If everything goes to plan you should now see \"pci-stub\" as the module in use from your \"lspci -nnk\" output for the card. \n\n\tKernel driver in use: pci-stub\n\nYou're all set. Now we can move on to installing the core software. \n\n-----------\n\n**Setup KVM/QEMU**\n\nWe need to install qemu first. \n\n    sudo pacman -S qemu\n\nNext we need the UEFI bios called OVMF. Look [here](https://www.kraxel.org/repos/jenkins/edk2/) and get the edk2.git-ovmf-x64-###.noarch.rpm file. Install rpmextract\n\n    sudo pacman -S rpmextract\n\nNext we'll extract the files and move them over as they had them. \n\n    [kemmler@arch ovmf]$ ls\n    edk2.git-ovmf-x64-0-20150916.b1214.g2f667c5.noarch.rpm\n    [kemmler@arch ovmf]$ rpmextract.sh edk2.git-ovmf-x64-0-20150916.b1214.g2f667c5.noarch.rpm \n    [kemmler@arch ovmf]$ ls\n    edk2.git-ovmf-x64-0-20150916.b1214.g2f667c5.noarch.rpm  usr\n    [kemmler@arch ovmf]$ sudo cp -R usr/share/* /usr/share/\n    [kemmler@arch ovmf]$ ls /usr/share/edk2.git/ovmf-x64/\n    OVMF_CODE-pure-efi.fd  OVMF_CODE-with-csm.fd  OVMF-pure-efi.fd  OVMF_VARS-pure-efi.fd  OVMF_VARS-with-csm.fd  OVMF-with-csm.fd  UefiShell.iso\n\nNow we need to create our vfio-bind script that will replace our pci-stub placeholder driver. I suggest putting it in /usr/bin/vfio-bind and then running \"chmod +x /usr/bin/vfio-bind\" \n\n    #!/bin/bash\n    \n    modprobe vfio-pci\n    \n    for dev in \"$@\"; do\n            vendor=$(cat /sys/bus/pci/devices/$dev/vendor)\n            device=$(cat /sys/bus/pci/devices/$dev/device)\n            if [ -e /sys/bus/pci/devices/$dev/driver ]; then\n                    echo $dev > /sys/bus/pci/devices/$dev/driver/unbind\n            fi\n            echo $vendor $device > /sys/bus/pci/drivers/vfio-pci/new_id\n    done\n\nNext we bind our gpu. Remember to bind the whole gpu if necessary which means both buses if present. **Replace your pci bus**\n\n    sudo vfio-bind 0000:0#:00.0 0000:0#:00.1\n\nVerify that vfio-bind is now in control of the gpu with \"lspci -nnk\" \n\n\tKernel driver in use: vfio-pci\n\nNow we can test it out and see if it works! Make sure to verify paths are correct, change your pci bus ID, and remove the second pci bus line if you only have one for your card. Throw this script in a file and run it like you did with the vfio-bind script. Once  you do that you should be able to switch the input on your monitor or KVM switch and be greeted with a black terminal with yellow text. This is the UEFI shell and means that everything is working wonderfully!\n\n    #!/bin/bash\n    \n    cp /usr/share/edk2.git/ovmf-x64/OVMF_VARS-pure-efi.fd /tmp/my_vars.fd\n    qemu-system-x86_64 \\\n      -enable-kvm \\\n      -m 2048 \\\n      -cpu host,kvm=off \\\n      -vga none \\\n      -device vfio-pci,host=02:00.0,multifunction=on \\\n      -device vfio-pci,host=02:00.1 \\\n      -drive if=pflash,format=raw,readonly,file=/usr/share/edk2.git/ovmf-x64/OVMF_CODE-pure-efi.fd \\\n      -drive if=pflash,format=raw,file=/tmp/my_vars.fd\n\n-------------------------\n\n**Setting Up Windows**\n\nNow that we have video out all we need to do next is change our script up and install windows to a disk or to a qcow2 container. I'll be doing the latter but can help with the former. You need a windows iso. I'm using windows 10 enterprise. You also need to get the VirtIO drivers from redhat. You can download them [here](https://fedoraproject.org/wiki/Windows_Virtio_Drivers#Direct_download). I am using the \"Latest virtio-win iso\" but stable should also be fine. First let's create our qcow2 container. I'm using a few params to increase performance, if you have any tips on better methods I'd be glad to hear it. Command below. Change 120G to whatever size you want your container to be. \n\n    qemu-img create -f qcow2 -o preallocation=metadata,compat=1.1,lazy_refcounts=on win.img 120G\n\nNext we modify our script to include the windows iso, virtio iso, and our new container. Once again, verify all path names. I'm also passing through my usb keyboard to the guest. This line begins with -usb. Find your usb device by using \"lsusb\". I'd recommend not passing through your mouse just yet so that if you fuck up you can simply close the black qemu window that pops up to get your keyboard back. Alterantively just hook up a second keyboard if you have one. I always keep a spare around in case windows hangs. Notice I'm using writeback cache on my qcow2 image. Remove that if you do not want it. \n\n    #!/bin/bash\n    \n    cp /usr/share/edk2.git/ovmf-x64/OVMF_VARS-pure-efi.fd /tmp/my_vars.fd\n    qemu-system-x86_64 \\\n      -enable-kvm \\\n      -m 2048 \\\n      -cpu host,kvm=off \\\n      -vga none \\\n      -usb -usbdevice host:1b1c:1b09 \\\n      -device vfio-pci,host=02:00.0,multifunction=on \\\n      -device vfio-pci,host=02:00.1 \\\n      -drive if=pflash,format=raw,readonly,file=/usr/share/edk2.git/ovmf-x64/OVMF_CODE-pure-efi.fd \\\n      -drive if=pflash,format=raw,file=/tmp/my_vars.fd \\\n      -device virtio-scsi-pci,id=scsi \\\n      -drive file=/home/kemmler/kvm/win10.iso,id=isocd,format=raw,if=none -device scsi-cd,drive=isocd \\\n      -drive file=/home/kemmler/kvm/win.img,id=disk,format=qcow2,if=none,cache=writeback -device scsi-hd,drive=disk \\\n      -drive file=/home/kemmler/kvm/virt.iso,id=virtiocd,if=none,format=raw -device ide-cd,bus=ide.1,drive=virtiocd\n\nOnce you boot you should see the \"press any key to boot from cd...\" if you miss it you'll eventually get dumped into a Shell> prompt. Just type \"exit\" hit enter, and navigate to \"Boot Manager\". Select the first SCSI device and you should get the prompt again. Go through and install windows as you normally would. When you get to the disk selection screen it will prompt you for a driver. Navigate to the virtio iso, virtscsi folder, Windows 8.1, x64, assuming you're using windows 10 x64 like me. Go through the rest of the install and then shut it down when done. Next we're going to get sound working, add our mouse to the usb passthrough, and set our monitors to switch automatically with xrandr. \n\nRun \"xrandr\" to find your output device names. They correspond to your connection types. eg dvi-d-0. I'm going to be using two monitors while in windows and leaving 1 for linux to keep conky running to monitor the system. My two monitors i'll be switching are called \"DVI-I-1\" and \"DVI-D-0\". I'm also changing the cpu values to match my 4790k and my ram to 8GB. Note the mode and the rate on the xrandr commands. This refers to the resolution and refresh rate respectively. \n\n    #!/bin/bash\n    \n    xrandr --output DVI-I-1 --off\n    xrandr --output DVI-D-0 --off\n    cp /usr/share/edk2.git/ovmf-x64/OVMF_VARS-pure-efi.fd /tmp/my_vars.fd\n    QEMU_PA_SAMPLES=128 QEMU_AUDIO_DRV=pa\n    qemu-system-x86_64 \\\n      -enable-kvm \\\n      -m 8196 \\\n      -smp cores=4,threads=2 \\\n      -cpu host,kvm=off \\\n      -vga none \\\n      -soundhw hda \\\n      -usb -usbdevice host:1b1c:1b09 -usbdevice host:046d:c07d \\\n      -device vfio-pci,host=02:00.0,multifunction=on \\\n      -device vfio-pci,host=02:00.1 \\\n      -drive if=pflash,format=raw,readonly,file=/usr/share/edk2.git/ovmf-x64/OVMF_CODE-pure-efi.fd \\\n      -drive if=pflash,format=raw,file=/tmp/my_vars.fd \\\n      -device virtio-scsi-pci,id=scsi \\\n      -drive file=/home/kemmler/kvm/win10.iso,id=isocd,format=raw,if=none -device scsi-cd,drive=isocd \\\n      -drive file=/home/kemmler/kvm/win.img,id=disk,format=qcow2,if=none,cache=writeback -device scsi-hd,drive=disk \\\n      -drive file=/home/kemmler/kvm/virt.iso,id=virtiocd,if=none,format=raw -device ide-cd,bus=ide.1,drive=virtiocd\n    xrandr --output DVI-D-0 --mode \"2560x1440\" --rate 60 --left-of HDMI-0\n    xrandr --output DVI-I-1 --mode \"1920x1080\" --rate 144 --left-of DVI-D-0\n\nFrom here you should be able to install the nvidia driver, steam, etc. You'll probably need to reboot to get the nvidia driver working. Once that's done everything should be great. \n\n----------------\n\n**Common Questions/Problems and Final Thoughts**\n\n1. Can I SLI in the guest VM? Short answer is probably not. Neither of my lga 1150 motherboards will allow me to try. \n2. Can I use two identical GPUs with one being in each OS? You will run into problems binding just one with pci-stub if both gpus share the same identifier found in \"lspci -nnk\". pci-stub will bind both of them and the nvidia driver will cease to function in linux. A potential workaround is to use xen's pciback module. This will allow you to grab based on the pci bus rather than the device id but I haven't tried this. \n3. Sound isn't working! I'm using pulseaudio and the hda device. Honestly you'll have to experiment. There's a 200+ page forum post with people posting working configs [here](https://bbs.archlinux.org/viewtopic.php?id=162768)\n4. The guide is too long or not easy enough. This guide isn't for you then.  \n5. Can I use X device? Generally you can pass through any pci, or usb device. If you want an actual answer the only way would be to donate me the part so I can figure it out short of finding someone on google that claims it works. \n\n--------------------\n\nHopefully you found this guide useful. I'm sure it seems like a gigantic pain in the ass but really once you set it up you don't have to mess with it again. I cannot stress how useful it is to simply be able to boot a VM play what I want and then turn it off without having to close all my shit by dual booting constantly. So let me know if this guide helped or how I can improve on it!",
  "mod_reports": [],
  "secure_media_embed": {},
  "edited": 1442802692.0,
  "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>New <a href=\"http://www.3dmark.com/3dm/8633319?\">Proof</a> &amp; <a href=\"https://www.youtube.com/watch?v=5bfUF9V7Uto\">Demo</a>. 14.5k Firestrike score on a 980ti in a VM when 15k is the norm. Roughly a 3.33% loss in performance. </p>\n\n<p>Hey everyone. I made a guide on how to do GPU passthrough 6 months ago located <a href=\"https://www.reddit.com/r/pcmasterrace/comments/2z0evz/gpu_passthrough_or_how_to_play_any_game_at_near/\">here</a> and figured it was time to revisit it. It&#39;s currently the 2nd result on google when you search for &quot;gpu passthrough&quot; and I still get messages quite frequently about it so for the sake of my sanity and others looking to do this I&#39;ll go ahead and give everyone a rundown of the best way to achieve this with current methods. This is meant to be a start to finish, <strong>holy shit this actually works</strong>, guide and is another lengthy post because there&#39;s a lot to cover so stick with it and you&#39;ll be happy you did. This post is going to cover UEFI specific hardware because every GPU made in the last few years has had it. I believe Nvidia implemented UEFI bios in the 600 series cards and some of you may need to flash the bios for that support, so all of you 500 series owners looking to pass them through will need to refer to the previous post and you want the q35 bios method. I&#39;m unsure on when exactly AMD implemented support so do your research, I&#39;m also supporting Intel/Nvidia exclusively in this post because I don&#39;t own any AMD hardware. Everyone else can continue reading. </p>\n\n<hr/>\n\n<p><strong>My Hardware Config</strong> </p>\n\n<p>CPU: i7 4790k</p>\n\n<p>For those that skipped the first tutorial you need VT-d. This is our bread and butter tech that allows us to pass through the GPU. </p>\n\n<p>Mobo: MSI z97a Gaming 7 </p>\n\n<p>It&#39;s a bit of an upgrade for me because my g1.sniper h6 was giving me fits when I upgraded to 32GB of ram. Manual doesn&#39;t say the MSI supports VT-d specifically but does say it supports &quot;virtualization technology&quot; which is what we&#39;re after. </p>\n\n<p>GPU: 1 x EVGA GTX 970 FTW, 1 x ASUS STRIX GTX 980ti</p>\n\n<p>I got a fancy 1440p 144hz monitor and the 970 wasn&#39;t cutting it so I picked up a 980ti. You <strong><em>do not</em></strong> need these expensive cards to achieve this. The 980ti supports UEFI. That&#39;s all we&#39;re after in this post. I&#39;ve done this with everything from a gtx 260, 550ti, 970, and now 980ti. </p>\n\n<p>Storage: 1 x Intel 730 240GB SSD, 4 x 1TB WD in Raid 10</p>\n\n<p>I changed my storage up a bit for peace of mind reasons. I&#39;m now running windows off of a qcow2 container rather than a physical hdd. You can do either! If you would like to run it off a physical disk refer to the previous post. </p>\n\n<p>RAM: 32GB of ddr3 1600</p>\n\n<p>Linux doesn&#39;t require much ram. I simply have 32GB because I run a lot of VM&#39;s for work and emulate an enterprise environment. Here&#39;s my current usage. You&#39;re free to allocate however much you want to windows or linux.</p>\n\n<pre><code>total     used\n32126     1399\n</code></pre>\n\n<p>Monitors: 2 x 1920x1080, 1 x 1440p </p>\n\n<p>The gist of monitor setups with this is to wire everything twice. You need one cable going out of each GPU to each monitor. If your monitor only has one input you can buy a switch for whatever connection you need. If you get a switch you can either leave both outputs enabled all the time or turn the linux output off with xrandr and the switch should failover automatically to the second input. If you don&#39;t have a switch then I would recommend using xrandr because it gets annoying to manually switch the inputs every time on the monitor. </p>\n\n<hr/>\n\n<p><strong>Setup and Installation</strong></p>\n\n<p>I&#39;m lazy as shit and break things constantly. So I&#39;m using Antergos to install Arch. I put my /home on the raid10 and just change /etc/fstab if I need to reinstall so I never lose anything important. Pretty much any offshoot of Arch and vanilla Arch should work just fine for this post. I have also helped people do this on Debian and derivatives. For this guide we will be sticking to Arch. I&#39;m using grub2 as my bootloader with UEFI.  </p>\n\n<p>The first thing we need to do is enable VT-d and ensure functionality. We need to edit /etc/default/grub . Look for the line that says &quot;GRUB_CMDLINE_DEFAULT=&quot;&quot; and append &quot;intel_iommu=on&quot; to what is inside of that line. Mine looks like this</p>\n\n<pre><code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;resume=UUID=6771936b-06b6-493c-b655-6f60122f5228 intel_iommu=on&quot;\n</code></pre>\n\n<p>Once you&#39;ve done that we need to rebuild our grub.cfg file. Run this to do so</p>\n\n<pre><code>sudo grub-mkconfig -o /boot/grub/grub.cfg\n</code></pre>\n\n<p>Next we reboot to activate iommu/vt-d. Once you&#39;re back in Arch we need to verify that VT-d is enabled and functioning. First we need to identify the pci-e bus the GPU we&#39;re passing through is on. We run &quot;lspci -nnk&quot; to find this information. Here are the lines important to me.  </p>\n\n<pre><code>02:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] [10de:17c8] (rev a1)\n    Subsystem: ASUSTeK Computer Inc. Device [1043:8548]\n    Kernel driver in use: nvidia\n    Kernel modules: nouveau, nvidia\n02:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:0fb0] (rev a1)\n    Subsystem: ASUSTeK Computer Inc. Device [1043:8548]\n    Kernel driver in use: snd_hda_in\n</code></pre>\n\n<p>My 980ti is in the second pci-e slot on my motherboard so this is correct. 02.00.1 is the audio bus for the card and is also important later on. Next we need to see if the 980ti falls into another pci-e bus&#39;s iommu group and if that will conflict. To find this out you look in /sys/bus/pci/devices/YOUR_BUS/iommu_group/devices/. If you do not have an iommu_group folder then vt-d was not enabled properly! Here is the output for mine</p>\n\n<pre><code>[kemmler@arch ~]$ ls -lha /sys/bus/pci/devices/0000\\:02\\:00.0/iommu_group/devices/\ntotal 0\ndrwxr-xr-x 2 root root 0 Sep 19 18:47 .\ndrwxr-xr-x 3 root root 0 Sep 19 18:46 ..\nlrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:00:01.0 -&gt; ../../../../devices/pci0000:00/0000:00:01.0\nlrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:00:01.1 -&gt; ../../../../devices/pci0000:00/0000:00:01.1\nlrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:01:00.0 -&gt; ../../../../devices/pci0000:00/0000:00:01.0/0000:01:00.0\nlrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:01:00.1 -&gt; ../../../../devices/pci0000:00/0000:00:01.0/0000:01:00.1\nlrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:02:00.0 -&gt; ../../../../devices/pci0000:00/0000:00:01.1/0000:02:00.0\nlrwxrwxrwx 1 root root 0 Sep 19 18:47 0000:02:00.1 -&gt; ../../../../devices/pci0000:00/0000:00:01.1/0000:02:00.1\n</code></pre>\n\n<p>0000:00:01.0 and 0000:00:01.1 can be ignored. The issue is 0000:01:00.0 and 0000:01:00.1. These are my 970 and will cause GPU passthrough to fail unless I pass both cards through to the VM. If you only see device of 0000:0#.00.# in your output then your iommu group is clean and you can skip this next section</p>\n\n<hr/>\n\n<p><strong>Fixing IOMMU Grouping</strong></p>\n\n<p>I installed Antergos with AUR support. This provides a tool called yaourt. A very thoughtful member of the Arch community has been steadily providing a current kernel patched with a few fixes that deal with IOMMU groups and other issues. This package is called &quot;linux-vfio&quot;. Assuming you have yaourt installed you will run this. </p>\n\n<pre><code>yaourt -S linux-vfio\n</code></pre>\n\n<p>It will ask you if you want to edit a few things, you can say no. It will then ask you if you want to only install linux-vfio. Say <strong>NO</strong> so it will also install the docs and headers for the kernel. Proceed through the installation with common sense. Once you&#39;ve installed it, rebuild your grub.cfg as above with &quot;sudo grub-mkconfig -o /boot/grub/grub.cfg&quot;. </p>\n\n<p>///</p>\n\n<p>If you installed the &quot;nvidia&quot; package from the arch repo your driver will probably break in the new kernel. I simply installed the binary from nvidia&#39;s website. A simple way to do that against the new kernel is to download the binary, reboot, edit grub by pressing &quot;e&quot; with linux-vfio selected, and then append &quot;nomodeset systemd.unit=multi-user.target&quot; to the end of the long line that begins with &quot;linux ...&quot; so you do a one time edit of the boot parameters. Then navigate to the binary and &quot;sh NVIDIA-###...sh&quot; It should disable nouveau if needed and install. Reboot and continue. You may also want to look into nvidia-dkms if you plan on updating your linux-vfio kernel regularly. </p>\n\n<p>///</p>\n\n<p>Once you&#39;re in the linux-vfio kernel you need to enable the acs_override patch. The easiest way is to just use the downstream method. There are other <a href=\"https://lkml.org/lkml/2013/5/30/513\">options</a> but should not be necessary. We will add &quot;pcie_acs_override=downstream&quot; to our grub.cfg at /etc/default/grub. &quot;sudo grub-mkconfig -o /boot/grub/grub.cfg&quot; once again to rebuild it. </p>\n\n<pre><code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;resume=UUID=6771936b-06b6-493c-b655-6f60122f5228 pcie_acs_override=downstream intel_iommu=on&quot;\n</code></pre>\n\n<p>Reboot and then check your iommu group again. </p>\n\n<pre><code>[kemmler@arch ~]$ ls -lha /sys/bus/pci/devices/0000\\:02\\:00.0/iommu_group/devices/\ntotal 0\ndrwxr-xr-x 2 root root 0 Sep 19 19:11 .\ndrwxr-xr-x 3 root root 0 Sep 19 19:11 ..\nlrwxrwxrwx 1 root root 0 Sep 19 19:11 0000:02:00.0 -&gt; ../../../../devices/pci0000:00/0000:00:01.1/0000:02:00.0\nlrwxrwxrwx 1 root root 0 Sep 19 19:11 0000:02:00.1 -&gt; ../../../../devices/pci0000:00/0000:00:01.1/0000:02:00.1\n</code></pre>\n\n<p>0000:01:00.0/1 are now missing from the initial output. This is exactly what we want to see. This means that the 980ti is now in it&#39;s own IOMMU group and can be allocated to a VM by itself. We&#39;re now ready to move on. </p>\n\n<hr/>\n\n<p><strong>Setup Continued</strong></p>\n\n<p>The next thing we need to do is blacklist the GPU we&#39;re passing through to the VM so that the Nvidia driver doesn&#39;t try to grab it. Nvidia is a dick and doesn&#39;t conform to standards properly. You can&#39;t easily unbind a gpu from the Nvidia driver so we use a module called &quot;pci-stub&quot; to claim the card before nvidia can. We achieve this by putting pci-stub in our initramfs that is loaded before the kernel and passing a parameter to grub telling it what to do. So, edit &quot;/etc/mkinitcpio.conf&quot; and add &quot;pci-stub&quot; to the Modules=&quot;&quot; section like so </p>\n\n<pre><code>MODULES=&quot;pci-stub&quot;\n</code></pre>\n\n<p>If you&#39;re running the stock kernel use this to rebuild your initramfs</p>\n\n<pre><code>sudo mkinitcpio -p linux\n</code></pre>\n\n<p>linux-vfio users run this</p>\n\n<pre><code>sudo mkinitcpio -p linux-vfio\n</code></pre>\n\n<p>Now we edit grub once again and add our pci_stub options to bind our card to pci-stub. Get your device IDs from &quot;lspci -nnk&quot;. My id&#39;s are &quot;10de:17c8&quot; and &quot;10de:0fb0&quot; as seen here again</p>\n\n<pre><code>02:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] [10de:17c8] (rev a1)\n    Subsystem: ASUSTeK Computer Inc. Device [1043:8548]\n    Kernel driver in use: nvidia\n    Kernel modules: nouveau, nvidia\n02:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:0fb0] (rev a1)\n    Subsystem: ASUSTeK Computer Inc. Device [1043:8548]\n    Kernel driver in use: snd_hda_intel\n    Kernel modules: snd_hda_intel\n</code></pre>\n\n<p>Edit /etc/default/grub and then run &quot;sudo grub-mkconfig -o /boot/grub/grub.cfg&quot; Your line should similar to this. Remember that you need to pass what&#39;s in the IOMMU group so you need the main card and the audio bus if it&#39;s there. </p>\n\n<pre><code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;resume=UUID=6771936b-06b6-493c-b655-6f60122f5228 pcie_acs_override=downstream intel_iommu=on pci-stub.ids=10de:17c8,10de:0fb0&quot;\n</code></pre>\n\n<p>Reboot. If everything goes to plan you should now see &quot;pci-stub&quot; as the module in use from your &quot;lspci -nnk&quot; output for the card. </p>\n\n<pre><code>Kernel driver in use: pci-stub\n</code></pre>\n\n<p>You&#39;re all set. Now we can move on to installing the core software. </p>\n\n<hr/>\n\n<p><strong>Setup KVM/QEMU</strong></p>\n\n<p>We need to install qemu first. </p>\n\n<pre><code>sudo pacman -S qemu\n</code></pre>\n\n<p>Next we need the UEFI bios called OVMF. Look <a href=\"https://www.kraxel.org/repos/jenkins/edk2/\">here</a> and get the edk2.git-ovmf-x64-###.noarch.rpm file. Install rpmextract</p>\n\n<pre><code>sudo pacman -S rpmextract\n</code></pre>\n\n<p>Next we&#39;ll extract the files and move them over as they had them. </p>\n\n<pre><code>[kemmler@arch ovmf]$ ls\nedk2.git-ovmf-x64-0-20150916.b1214.g2f667c5.noarch.rpm\n[kemmler@arch ovmf]$ rpmextract.sh edk2.git-ovmf-x64-0-20150916.b1214.g2f667c5.noarch.rpm \n[kemmler@arch ovmf]$ ls\nedk2.git-ovmf-x64-0-20150916.b1214.g2f667c5.noarch.rpm  usr\n[kemmler@arch ovmf]$ sudo cp -R usr/share/* /usr/share/\n[kemmler@arch ovmf]$ ls /usr/share/edk2.git/ovmf-x64/\nOVMF_CODE-pure-efi.fd  OVMF_CODE-with-csm.fd  OVMF-pure-efi.fd  OVMF_VARS-pure-efi.fd  OVMF_VARS-with-csm.fd  OVMF-with-csm.fd  UefiShell.iso\n</code></pre>\n\n<p>Now we need to create our vfio-bind script that will replace our pci-stub placeholder driver. I suggest putting it in /usr/bin/vfio-bind and then running &quot;chmod +x /usr/bin/vfio-bind&quot; </p>\n\n<pre><code>#!/bin/bash\n\nmodprobe vfio-pci\n\nfor dev in &quot;$@&quot;; do\n        vendor=$(cat /sys/bus/pci/devices/$dev/vendor)\n        device=$(cat /sys/bus/pci/devices/$dev/device)\n        if [ -e /sys/bus/pci/devices/$dev/driver ]; then\n                echo $dev &gt; /sys/bus/pci/devices/$dev/driver/unbind\n        fi\n        echo $vendor $device &gt; /sys/bus/pci/drivers/vfio-pci/new_id\ndone\n</code></pre>\n\n<p>Next we bind our gpu. Remember to bind the whole gpu if necessary which means both buses if present. <strong>Replace your pci bus</strong></p>\n\n<pre><code>sudo vfio-bind 0000:0#:00.0 0000:0#:00.1\n</code></pre>\n\n<p>Verify that vfio-bind is now in control of the gpu with &quot;lspci -nnk&quot; </p>\n\n<pre><code>Kernel driver in use: vfio-pci\n</code></pre>\n\n<p>Now we can test it out and see if it works! Make sure to verify paths are correct, change your pci bus ID, and remove the second pci bus line if you only have one for your card. Throw this script in a file and run it like you did with the vfio-bind script. Once  you do that you should be able to switch the input on your monitor or KVM switch and be greeted with a black terminal with yellow text. This is the UEFI shell and means that everything is working wonderfully!</p>\n\n<pre><code>#!/bin/bash\n\ncp /usr/share/edk2.git/ovmf-x64/OVMF_VARS-pure-efi.fd /tmp/my_vars.fd\nqemu-system-x86_64 \\\n  -enable-kvm \\\n  -m 2048 \\\n  -cpu host,kvm=off \\\n  -vga none \\\n  -device vfio-pci,host=02:00.0,multifunction=on \\\n  -device vfio-pci,host=02:00.1 \\\n  -drive if=pflash,format=raw,readonly,file=/usr/share/edk2.git/ovmf-x64/OVMF_CODE-pure-efi.fd \\\n  -drive if=pflash,format=raw,file=/tmp/my_vars.fd\n</code></pre>\n\n<hr/>\n\n<p><strong>Setting Up Windows</strong></p>\n\n<p>Now that we have video out all we need to do next is change our script up and install windows to a disk or to a qcow2 container. I&#39;ll be doing the latter but can help with the former. You need a windows iso. I&#39;m using windows 10 enterprise. You also need to get the VirtIO drivers from redhat. You can download them <a href=\"https://fedoraproject.org/wiki/Windows_Virtio_Drivers#Direct_download\">here</a>. I am using the &quot;Latest virtio-win iso&quot; but stable should also be fine. First let&#39;s create our qcow2 container. I&#39;m using a few params to increase performance, if you have any tips on better methods I&#39;d be glad to hear it. Command below. Change 120G to whatever size you want your container to be. </p>\n\n<pre><code>qemu-img create -f qcow2 -o preallocation=metadata,compat=1.1,lazy_refcounts=on win.img 120G\n</code></pre>\n\n<p>Next we modify our script to include the windows iso, virtio iso, and our new container. Once again, verify all path names. I&#39;m also passing through my usb keyboard to the guest. This line begins with -usb. Find your usb device by using &quot;lsusb&quot;. I&#39;d recommend not passing through your mouse just yet so that if you fuck up you can simply close the black qemu window that pops up to get your keyboard back. Alterantively just hook up a second keyboard if you have one. I always keep a spare around in case windows hangs. Notice I&#39;m using writeback cache on my qcow2 image. Remove that if you do not want it. </p>\n\n<pre><code>#!/bin/bash\n\ncp /usr/share/edk2.git/ovmf-x64/OVMF_VARS-pure-efi.fd /tmp/my_vars.fd\nqemu-system-x86_64 \\\n  -enable-kvm \\\n  -m 2048 \\\n  -cpu host,kvm=off \\\n  -vga none \\\n  -usb -usbdevice host:1b1c:1b09 \\\n  -device vfio-pci,host=02:00.0,multifunction=on \\\n  -device vfio-pci,host=02:00.1 \\\n  -drive if=pflash,format=raw,readonly,file=/usr/share/edk2.git/ovmf-x64/OVMF_CODE-pure-efi.fd \\\n  -drive if=pflash,format=raw,file=/tmp/my_vars.fd \\\n  -device virtio-scsi-pci,id=scsi \\\n  -drive file=/home/kemmler/kvm/win10.iso,id=isocd,format=raw,if=none -device scsi-cd,drive=isocd \\\n  -drive file=/home/kemmler/kvm/win.img,id=disk,format=qcow2,if=none,cache=writeback -device scsi-hd,drive=disk \\\n  -drive file=/home/kemmler/kvm/virt.iso,id=virtiocd,if=none,format=raw -device ide-cd,bus=ide.1,drive=virtiocd\n</code></pre>\n\n<p>Once you boot you should see the &quot;press any key to boot from cd...&quot; if you miss it you&#39;ll eventually get dumped into a Shell&gt; prompt. Just type &quot;exit&quot; hit enter, and navigate to &quot;Boot Manager&quot;. Select the first SCSI device and you should get the prompt again. Go through and install windows as you normally would. When you get to the disk selection screen it will prompt you for a driver. Navigate to the virtio iso, virtscsi folder, Windows 8.1, x64, assuming you&#39;re using windows 10 x64 like me. Go through the rest of the install and then shut it down when done. Next we&#39;re going to get sound working, add our mouse to the usb passthrough, and set our monitors to switch automatically with xrandr. </p>\n\n<p>Run &quot;xrandr&quot; to find your output device names. They correspond to your connection types. eg dvi-d-0. I&#39;m going to be using two monitors while in windows and leaving 1 for linux to keep conky running to monitor the system. My two monitors i&#39;ll be switching are called &quot;DVI-I-1&quot; and &quot;DVI-D-0&quot;. I&#39;m also changing the cpu values to match my 4790k and my ram to 8GB. Note the mode and the rate on the xrandr commands. This refers to the resolution and refresh rate respectively. </p>\n\n<pre><code>#!/bin/bash\n\nxrandr --output DVI-I-1 --off\nxrandr --output DVI-D-0 --off\ncp /usr/share/edk2.git/ovmf-x64/OVMF_VARS-pure-efi.fd /tmp/my_vars.fd\nQEMU_PA_SAMPLES=128 QEMU_AUDIO_DRV=pa\nqemu-system-x86_64 \\\n  -enable-kvm \\\n  -m 8196 \\\n  -smp cores=4,threads=2 \\\n  -cpu host,kvm=off \\\n  -vga none \\\n  -soundhw hda \\\n  -usb -usbdevice host:1b1c:1b09 -usbdevice host:046d:c07d \\\n  -device vfio-pci,host=02:00.0,multifunction=on \\\n  -device vfio-pci,host=02:00.1 \\\n  -drive if=pflash,format=raw,readonly,file=/usr/share/edk2.git/ovmf-x64/OVMF_CODE-pure-efi.fd \\\n  -drive if=pflash,format=raw,file=/tmp/my_vars.fd \\\n  -device virtio-scsi-pci,id=scsi \\\n  -drive file=/home/kemmler/kvm/win10.iso,id=isocd,format=raw,if=none -device scsi-cd,drive=isocd \\\n  -drive file=/home/kemmler/kvm/win.img,id=disk,format=qcow2,if=none,cache=writeback -device scsi-hd,drive=disk \\\n  -drive file=/home/kemmler/kvm/virt.iso,id=virtiocd,if=none,format=raw -device ide-cd,bus=ide.1,drive=virtiocd\nxrandr --output DVI-D-0 --mode &quot;2560x1440&quot; --rate 60 --left-of HDMI-0\nxrandr --output DVI-I-1 --mode &quot;1920x1080&quot; --rate 144 --left-of DVI-D-0\n</code></pre>\n\n<p>From here you should be able to install the nvidia driver, steam, etc. You&#39;ll probably need to reboot to get the nvidia driver working. Once that&#39;s done everything should be great. </p>\n\n<hr/>\n\n<p><strong>Common Questions/Problems and Final Thoughts</strong></p>\n\n<ol>\n<li>Can I SLI in the guest VM? Short answer is probably not. Neither of my lga 1150 motherboards will allow me to try. </li>\n<li>Can I use two identical GPUs with one being in each OS? You will run into problems binding just one with pci-stub if both gpus share the same identifier found in &quot;lspci -nnk&quot;. pci-stub will bind both of them and the nvidia driver will cease to function in linux. A potential workaround is to use xen&#39;s pciback module. This will allow you to grab based on the pci bus rather than the device id but I haven&#39;t tried this. </li>\n<li>Sound isn&#39;t working! I&#39;m using pulseaudio and the hda device. Honestly you&#39;ll have to experiment. There&#39;s a 200+ page forum post with people posting working configs <a href=\"https://bbs.archlinux.org/viewtopic.php?id=162768\">here</a></li>\n<li>The guide is too long or not easy enough. This guide isn&#39;t for you then.<br/></li>\n<li>Can I use X device? Generally you can pass through any pci, or usb device. If you want an actual answer the only way would be to donate me the part so I can figure it out short of finding someone on google that claims it works. </li>\n</ol>\n\n<hr/>\n\n<p>Hopefully you found this guide useful. I&#39;m sure it seems like a gigantic pain in the ass but really once you set it up you don&#39;t have to mess with it again. I cannot stress how useful it is to simply be able to boot a VM play what I want and then turn it off without having to close all my shit by dual booting constantly. So let me know if this guide helped or how I can improve on it!</p>\n</div><!-- SC_ON -->",
  "downs": 0,
  "user_reports": [],
  "over_18": false,
  "score": 99,
  "stickied": false,
  "domain": "self.linuxmasterrace",
  "is_self": true,
  "hidden": false,
  "_orphaned": {},
  "_api_link": "https://api.reddit.com/r/linuxmasterrace/comments/3lnorm/gpu_passthrough_revisited_an_updated_guide_on_how/?ref=search_posts",
  "_comments_by_id": {},
  "visited": false,
  "thumbnail": "self",
  "_has_fetched": true,
  "gilded": 0,
  "hide_score": false,
  "_info_url": "https://api.reddit.com/api/info/",
  "created_utc": 1442738531.0,
  "subreddit_id": "t5_2ykcc",
  "id": "3lnorm",
  "_params": {},
  "media_embed": {},
  "name": "t3_3lnorm",
  "quarantine": false,
  "permalink": "https://www.reddit.com/r/linuxmasterrace/comments/3lnorm/gpu_passthrough_revisited_an_updated_guide_on_how/?ref=search_posts",
  "ups": 99,
  "url": "https://www.reddit.com/r/linuxmasterrace/comments/3lnorm/gpu_passthrough_revisited_an_updated_guide_on_how/",
  "title": "GPU Passthrough Revisited - An Updated Guide On How To Game In A Virtual Machine (xpost /r/pcmasterrace)",
  "preview": {
    "images": [
      {
        "resolutions": [],
        "id": "EdaQxJeXXbDAR7IH6sBO_A4JGYRzpN5CoV5gk49NIGo",
        "variants": {},
        "source": {
          "width": 99,
          "height": 100,
          "url": "https://i.redditmedia.com/eGlCBaAZxaZ4C0S1tU0TBdY2fqaRbU4lwWWcHRRLtY4.jpg?s=7541bbb84f595b231e21083204e546a2"
        }
      }
    ]
  }
}