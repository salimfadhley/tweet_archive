{
  "downs": 0,
  "mod_reports": [],
  "secure_media_embed": {},
  "_info_url": "https://api.reddit.com/api/info/",
  "gilded": 0,
  "num_comments": 108,
  "_orphaned": {},
  "hidden": false,
  "ups": 204,
  "url": "https://www.reddit.com/r/linux_gaming/comments/2z2y7h/gpu_passthrough_or_how_to_play_any_game_at_near/",
  "_has_fetched": true,
  "quarantine": false,
  "name": "t3_2z2y7h",
  "user_reports": [],
  "saved": false,
  "permalink": "https://www.reddit.com/r/linux_gaming/comments/2z2y7h/gpu_passthrough_or_how_to_play_any_game_at_near/?ref=search_posts",
  "over_18": false,
  "is_self": true,
  "_params": {},
  "thumbnail": "",
  "created_utc": 1426384599.0,
  "_replaced_more": false,
  "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p><strong><a href=\"https://www.reddit.com/r/pcmasterrace/comments/3lno0t/gpu_passthrough_revisited_an_updated_guide_on_how/\">Updated Guide Is Here</a></strong></p>\n\n<hr/>\n\n<p>\ufeffFirst, a <a href=\"https://www.youtube.com/watch?v=37D2bRsthfI\">demonstration</a> and <a href=\"http://www.3dmark.com/3dm/5992909\">proof</a></p>\n\n<p>So, I&#39;ve been reading a steady rise in posts about people wanting to switch to Linux. Everyone seems to think they only have three options. 1) Run Windows exclusively,  2) Run Linux exclusively, or 3) Dual boot Windows and Linux. Well, I&#39;m here to tell you that there&#39;s a 4th option. Here&#39;s the problem. It&#39;s going to be tough for a lot of you. I managed to set it up in about 4 days and I&#39;ve been using Linux for about a decade. How you go about it will depend entirely on your setup. I&#39;ll explain mine now. </p>\n\n<p>CPU: i7-4790k</p>\n\n<p>You need a CPU that supports IOMMU which is VT-D (Intel) and AMD-Vi. This allows Virtual Machines to interface directly with peripheral devices. Currently every 2011-V3 cpu, i7-4790k, and most non-K processors do. \nFor a mostly complete list of Intel/AMD cpus that support IOMMU check <a href=\"https://en.wikipedia.org/wiki/List_of_IOMMU-supporting_hardware\">here</a>\nIf yours doesn&#39;t and you don&#39;t plan to get one anytime soon then this isn&#39;t for you.</p>\n\n<p>GPU: 1 x GTX 970 , 1 x GTX 260 , 1 x i7 Integrated Graphics</p>\n\n<p>I&#39;m going to preface this by saying my setup is weird. I plan to replace the 260 and stop using the IGP as soon as I can afford another 970. My GTX 260 powers two monitors with the IGP powering the third. This is a limitation of the 260 since it only supports two monitors. The GTX 970 does nothing until I start the Virtual Machine. When I start the Virtual Machine the 970&#39;s fans spin up exactly like they would when you normally turn on a computer and it is allocated solely to the Virtual Machine. Once I replace my 260 I will be able to SLI the 970s in Windows or Linux or use one for each OS. It will be wonderful. Also note that later on you&#39;ll see me using OVMF BIOS. These are UEFI only. If your GPU does not support UEFI (yes that&#39;s a thing) then you will need to use the older i440fx or q35 bios. NOTE: These BIOS are used by the VM only. You don&#39;t have to flash or change your mobo&#39;s bios. </p>\n\n<p>Motherboard: Gigabyte g1 sniper h6</p>\n\n<p>I got this motherboard for very cheap which is why I&#39;m using it. It doesn&#39;t have dual 3.0 x16&#39;s so I&#39;ll have to change it once I get the second 970. <strong>IMPORTANT</strong> your motherboard must support IOMMU as well. The link above has a list of IOMMU supporting boards. </p>\n\n<p>Storage: 1 x Intel 730 240GB SSD , 1 x 300GB WD Velociraptor , 1 x 1TB WD </p>\n\n<p>I use the SSD as the main Linux drive with minor exceptions to the partition scheme. I have put /var and /home on the terabyte drive to reduce writes to the SSD. The velociraptor is used exclusively for the Windows Virtual Machine. I can also boot directly to it if needed but rarely do. Both the Linux and Windows OS&#39;s use UEFI as mentioned above and below. </p>\n\n<p>RAM: 16GB of ddr3 1600 G.Skill</p>\n\n<p>Your RAM specs aren&#39;t particularly relevant granted that you have enough to run Linux and Windows at the same time. You could easily get by with 8GB if you manage it. </p>\n\n<p>Monitors: The primary monitor I&#39;m going to be using has two DVI inputs. I have it connected to both the GTX 970 and the 260. You can do this with only one monitor! Once I disable the 260 input with xrandr as seen below the monitor with go into autoselect mode and switch to the second DVI port for me. The other two monitors are connected to the 260 and the IGP. </p>\n\n<hr/>\n\n<p>Still with me? Ok! I am a long time Debian user but found Arch much easier to do this with given the vast amount of support and readily available documentation. I&#39;ve read success stories on Ubuntu/KVM with varied Nvidia cards, Xen with AMD/Nvidia cards, and Arch with several Nvidia/AMD cards. <strong>Your goto guide is <a href=\"https://bbs.archlinux.org/viewtopic.php?id=162768\">here</a> if using Arch/Nvidia or AMD</strong> There are hundreds of posts and success stories. I&#39;m going to share how I accomplished mine now. This is about to get very technical so prepare your anus and get google ready to check up on any files/topics I don&#39;t explain enough for you.  I am going to explain the majority but the forum post covers much more. You do not have to install Arch by hand. This will probably piss some die hard Arch fans off but you can do this perfectly fine using Antergos and probably Manjero. Both are automated installers and I personally used and had success using Antergos with Cinnamon. </p>\n\n<hr/>\n\n<p>First up is our bread and butter, the kernel. I am using the supplied Kernel in OP&#39;s post described <a href=\"https://bbs.archlinux.org/viewtopic.php?id=162768\">here</a> and linked to <a href=\"https://drive.google.com/open?id=0Bxp_MsrVrNnEbU5tWkY4cldzM0E&amp;authuser=0\">here</a>. To compile the kernel, and subsequently install it you use this</p>\n\n<pre><code>tar -xvzf linux-mainline-3.18.5.tar.gz\ncd linux-mainline\nmakepkg\n</code></pre>\n\n<p>You&#39;re probably thinking that it&#39;s too easy but that&#39;s actually all you need to do. If you require any special kernel configs be sure to change those when the makeconfig pops up but for most users you shouldn&#39;t. Once that&#39;s done you will have 3 files that end in .pkg.tar.xz. You will need to install all 3. First, install the headers, the docs, then the kernel like so. </p>\n\n<pre><code>pacman -U linux-mainline-headers-3.18.5-1-x86_64.pkg.tar.xz\npacman -U linux-mainline-docs-3.18.5-1-x86_64.pkg.tar.xzz\npacman -U linux-mainline-3.18.5-1-x86_64.pkg.tar.xz\n</code></pre>\n\n<p>Once that&#39;s done we&#39;ll need to add the pci-stub module to initramfs. Vi or nano /etc/mkinitcpio.conf and change MODULES=&quot;&quot; to this MODULES=&quot;pci-stub&quot;. Next run mkinitcpio to update our initramfs</p>\n\n<pre><code>mkinitcpio -p linux-mainline\n</code></pre>\n\n<p>Now that we&#39;ve updated our initramfs we need to update grub. This part may be the most difficult simply because you will need to do independent research and decide which options are appropriate for you. First, here is my grub config. This file is stored in /etc/default/grub</p>\n\n<pre><code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;pcie_acs_override=downstream intel_iommu=on pci-stub.ids=10de:13c2,10de:0fbb&quot;\n</code></pre>\n\n<p>pcie_acs_override=downstream fixes an issue on my motherboard with IOMMU groups. If you leave this out and KVM gripes about not all devices being accessible then you may need this enabled. A full writeup on the subject can be found <a href=\"http://vfio.blogspot.com/2014/08/iommu-groups-inside-and-out.html\">here</a>. intel_iommu=on ensures that IOMMU is enabled within the kernel. pci-stub.ids=.. is our main concern. This has to be ran before the kernel is loaded so that the GPU is not allocated by the kernel allowing us to allocate it to a VM. Remember us adding pci-stub to initramfs? We will be blacklisting these cards using the device codes shown by lspci. Here is example output</p>\n\n<pre><code>[archie@arch kernel]$ lspci -nn |grep -i nvidia\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM204 [GeForce GTX 970] [10de:13c2] (rev a1)\n01:00.1 Audio device [0403]: NVIDIA Corporation GM204 High Definition Audio Controller [10de:0fbb] (rev a1)\n06:00.0 VGA compatible controller [0300]: NVIDIA Corporation GT200 [GeForce GTX 260] [10de:05e2] (rev a1)\n</code></pre>\n\n<p>Note that my device codes are 10de:13c2 and 10de:0fbb. 01:00.0 and 01:00.1 are defined by the kernel and respective to the pci bus location. 01 being my first PCI-E slot. So, since we have two device codes that we need to blacklist we use the comma delimiter to differentiate the two like so. <strong>NOTE: THIS WILL BLACKLIST YOUR GPU FROM THE KERNEL</strong></p>\n\n<pre><code>pci-stub.ids=10de:13c2,10de:0fbb\n</code></pre>\n\n<p>Now that we&#39;re done with that we need to update grub. It should take care of everything for you and essentially gives you 4 kernels. Linux-mainline, a fallback, and your old kernel with a fallback. </p>\n\n<pre><code>grub-mkconfig -o /boot/grub/grub.cfg\n</code></pre>\n\n<p>Verify that your pci-stub and other options are loaded by looking at /boot/grub/grub.cfg. You should see linux /vmlinuz-linux-mainline .. and then your options. If you are satisfied then reboot the machine. Once you have loaded back up run a &quot;dmesg |grep pci-stub&quot; and look for something similar to this. The device codes should match what you put in grub. </p>\n\n<pre><code>[    2.096151] pci-stub: add 1002:6719 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000\n[    2.096160] pci-stub 0000:07:00.0: claimed by stub\n[    2.096165] pci-stub: add 1002:AA80 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000\n[    2.096174] pci-stub 0000:07:00.1: claimed by stub\n[    2.096178] pci-stub: add 1B21:1042 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000\n</code></pre>\n\n<p>Assuming all is well we simply need to start our virtual machine. You&#39;ll need to install a few components now. You&#39;re going to need qemu and ovmf-svn. The latter can be pulled from the AUR <a href=\"https://aur.archlinux.org/packages/ovmf-svn\">here</a></p>\n\n<pre><code>pacman -Syy\npacman -S qemu\nwget https://aur.archlinux.org/packages/ov/ovmf-svn/ovmf-svn.tar.gz\ntar -xvzf ovmf-svn.tar.gz &amp;&amp; cd ovmf-svn\nmakepkg\npacman -U ovmf-svn-17048-1-x86_64.pkg.tar.xz\n</code></pre>\n\n<p>Once done we should just have to do a little scripting to set everything up. Almost done! We need to bind the gpu with vfio-pci. nbhs kindly provides us with a script to do just that,. Save it under /usr/bin/vfio-bind and make sure to chmod 755 /usr/bin/vfio-bind so we can use it!</p>\n\n<pre><code>#!/bin/bash\n\nmodprobe vfio-pci\n\nfor dev in &quot;$@&quot;; do\n        vendor=$(cat /sys/bus/pci/devices/$dev/vendor)\n        device=$(cat /sys/bus/pci/devices/$dev/device)\n        if [ -e /sys/bus/pci/devices/$dev/driver ]; then\n                echo $dev &gt; /sys/bus/pci/devices/$dev/driver/unbind\n        fi\n        echo $vendor $device &gt; /sys/bus/pci/drivers/vfio-pci/new_id\ndone\n</code></pre>\n\n<p>To bind our GPU we simply use this. Remember that my GPU is located at 01.00.0 and 01.00.1 so adjust yours accordingly. Check with lspci -nn</p>\n\n<pre><code>vfio-bind 0000:01:00.0 0000:01:00.1\n</code></pre>\n\n<p>Now, you should be able to test using the i440fx but if it doesn&#39;t work don&#39;t be disheartened. If you see output then fantastic, otherwise keep reading and try the later steps as well. I have a 4790k which means I have 4 cores and 2 threads per core, adjust accordingly. Also be sure to change the device ID in the vfio-pci options. </p>\n\n<pre><code>qemu-system-x86_64 -enable-kvm -m 1024 -cpu host \\\n-smp cores=4,threads=2 \\\n-device vfio-pci,host=01:00.0,x-vga=on \\\n-device vfio-pci,host=01:00.1 \\\n-vga none \n</code></pre>\n\n<p>Now, I never got the i440fx to work. So I&#39;ll share my OVMF method. This is the full unadulterated script which I&#39;ll break down afterwards. </p>\n\n<pre><code>#!/bin/bash\n\nxrandr --output DVI-I-1 --off\nQEMU_PA_SAMPLES=128 QEMU_AUDIO_DRV=pa\nqemu-system-x86_64 -enable-kvm -m 8192 -cpu host,kvm=off -smp cores=4,threads=2 \\\n-pflash /usr/share/ovmf/x64/ovmf_x64.bin \\\n-vga none \\\n-usb -usbdevice host:1b1c:1b09 -usbdevice host:046d:c07d \\\n-soundhw hda \\\n-device vfio-pci,host=01:00.0,multifunction=on \\\n-device vfio-pci,host=01:00.1 \\\n-device virtio-scsi-pci,id=scsi \\\n-drive file=/home/tully/VMs/win10.iso,id=isocd,format=raw,if=none -device scsi-cd,drive=isocd \\\n-drive file=/dev/sdb,id=disk,format=raw,if=none -device scsi-hd,drive=disk \\\n-drive file=/home/tully/VMs/virt.iso,id=virtiocd,if=none -device ide-cd,bus=ide.1,drive=virtiocd \\\n-boot menu=on\npkill conky\nfixmonitors\nconky -d\n</code></pre>\n\n<p>First we kill my left monitor. This puts it into autodetect mode. It sees the 970 outputting video and switches to that input automagically. Next I set some environment variables to get sound working. Everything from qemu-system-x86_64 to -boot menu=on is one command. --enable-kvm tells qemu to use the kvm module, -m 8192 allocates 8192MB of RAM to the guest VM, kvm=off is a built in feature that attempts to hide the fact that any virtual machine stuff is going on. The Nvidia driver will code 43 if it detects a hypervisor. (they want you to buy quadros), the 4970k has 4 cores and 2 threads per core, -plfash tells qemu to use the OVMF bios, -vga none says to not emulate video, -usb steals my keyboard and mouse and passes it through to the VM, -soundhw sets our sound, -device vfio-pci sets our gpu up with qemu, -device virtio-scsi sets up the OVMF devices, first -drive is the windows 10 iso i used to install, second is my velociraptor which is located using fdisk -l, third is the virtio driver used to get windows to work with the OVMF devices and can be found <a href=\"https://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/\">here</a>. Set -boot menu=on so you can change the boot options if needed. Once you load the Windows iso it will complain that it can&#39;t find a drive. Just click the load driver button, navigate through the iso to find Windows 7 or 8.1 and select the only option it gives you. Should show your hdd fine after that. </p>\n\n<p>Everything else is just personally needed and runs after the guest VM shuts down. I kill conky because turning the monitor off and on messes up the positioning, fixmonitors is a script I wrote to get my monitors working again with xrandr and is pretty much just dependent on your setup and conky -d just starts conky again. </p>\n\n<hr/>\n\n<p>Wow that was a lot of typing. I&#39;m most likely going to be going back over this and revising as needed but it should serve as a good starting point for many looking to do this. Once you have everything set up you won&#39;t have to touch it again. It seems like a huge pain in the ass but to get away from the dependence of Windows for my everyday activities is worth far more effort in my opinion. Thanks everyone for sticking with me, please let me know if you see any mistakes or information you&#39;d like added, and I&#39;ll be happy to answer any questions you have. </p>\n\n<p>TL;DR - Linux is awesome</p>\n\n<p>Edit: removed mention of the vga arbitration patch as it is not needed with the ovmf bios. </p>\n</div><!-- SC_ON -->",
  "subreddit_id": "t5_2r2u0",
  "archived": true,
  "domain": "self.linux_gaming",
  "clicked": false,
  "score": 204,
  "id": "2z2y7h",
  "stickied": false,
  "media_embed": {},
  "visited": false,
  "selftext": "**[Updated Guide Is Here](https://www.reddit.com/r/pcmasterrace/comments/3lno0t/gpu_passthrough_revisited_an_updated_guide_on_how/)**\n\n------------------------------\n\n\n\ufeffFirst, a [demonstration](https://www.youtube.com/watch?v=37D2bRsthfI) and [proof](http://www.3dmark.com/3dm/5992909)\n\nSo, I've been reading a steady rise in posts about people wanting to switch to Linux. Everyone seems to think they only have three options. 1) Run Windows exclusively,  2) Run Linux exclusively, or 3) Dual boot Windows and Linux. Well, I'm here to tell you that there's a 4th option. Here's the problem. It's going to be tough for a lot of you. I managed to set it up in about 4 days and I've been using Linux for about a decade. How you go about it will depend entirely on your setup. I'll explain mine now. \n\nCPU: i7-4790k\n\nYou need a CPU that supports IOMMU which is VT-D (Intel) and AMD-Vi. This allows Virtual Machines to interface directly with peripheral devices. Currently every 2011-V3 cpu, i7-4790k, and most non-K processors do. \nFor a mostly complete list of Intel/AMD cpus that support IOMMU check [here](https://en.wikipedia.org/wiki/List_of_IOMMU-supporting_hardware)\nIf yours doesn't and you don't plan to get one anytime soon then this isn't for you.\n\nGPU: 1 x GTX 970 , 1 x GTX 260 , 1 x i7 Integrated Graphics\n\nI'm going to preface this by saying my setup is weird. I plan to replace the 260 and stop using the IGP as soon as I can afford another 970. My GTX 260 powers two monitors with the IGP powering the third. This is a limitation of the 260 since it only supports two monitors. The GTX 970 does nothing until I start the Virtual Machine. When I start the Virtual Machine the 970's fans spin up exactly like they would when you normally turn on a computer and it is allocated solely to the Virtual Machine. Once I replace my 260 I will be able to SLI the 970s in Windows or Linux or use one for each OS. It will be wonderful. Also note that later on you'll see me using OVMF BIOS. These are UEFI only. If your GPU does not support UEFI (yes that's a thing) then you will need to use the older i440fx or q35 bios. NOTE: These BIOS are used by the VM only. You don't have to flash or change your mobo's bios. \n\nMotherboard: Gigabyte g1 sniper h6\n\nI got this motherboard for very cheap which is why I'm using it. It doesn't have dual 3.0 x16's so I'll have to change it once I get the second 970. **IMPORTANT** your motherboard must support IOMMU as well. The link above has a list of IOMMU supporting boards. \n\nStorage: 1 x Intel 730 240GB SSD , 1 x 300GB WD Velociraptor , 1 x 1TB WD \n\nI use the SSD as the main Linux drive with minor exceptions to the partition scheme. I have put /var and /home on the terabyte drive to reduce writes to the SSD. The velociraptor is used exclusively for the Windows Virtual Machine. I can also boot directly to it if needed but rarely do. Both the Linux and Windows OS's use UEFI as mentioned above and below. \n\nRAM: 16GB of ddr3 1600 G.Skill\n\nYour RAM specs aren't particularly relevant granted that you have enough to run Linux and Windows at the same time. You could easily get by with 8GB if you manage it. \n\nMonitors: The primary monitor I'm going to be using has two DVI inputs. I have it connected to both the GTX 970 and the 260. You can do this with only one monitor! Once I disable the 260 input with xrandr as seen below the monitor with go into autoselect mode and switch to the second DVI port for me. The other two monitors are connected to the 260 and the IGP. \n\n---------------------\n\nStill with me? Ok! I am a long time Debian user but found Arch much easier to do this with given the vast amount of support and readily available documentation. I've read success stories on Ubuntu/KVM with varied Nvidia cards, Xen with AMD/Nvidia cards, and Arch with several Nvidia/AMD cards. **Your goto guide is [here](https://bbs.archlinux.org/viewtopic.php?id=162768) if using Arch/Nvidia or AMD** There are hundreds of posts and success stories. I'm going to share how I accomplished mine now. This is about to get very technical so prepare your anus and get google ready to check up on any files/topics I don't explain enough for you.  I am going to explain the majority but the forum post covers much more. You do not have to install Arch by hand. This will probably piss some die hard Arch fans off but you can do this perfectly fine using Antergos and probably Manjero. Both are automated installers and I personally used and had success using Antergos with Cinnamon. \n\n---------------------\n\nFirst up is our bread and butter, the kernel. I am using the supplied Kernel in OP's post described [here](https://bbs.archlinux.org/viewtopic.php?id=162768) and linked to [here](https://drive.google.com/open?id=0Bxp_MsrVrNnEbU5tWkY4cldzM0E&authuser=0). To compile the kernel, and subsequently install it you use this\n\n    tar -xvzf linux-mainline-3.18.5.tar.gz\n    cd linux-mainline\n    makepkg\n\nYou're probably thinking that it's too easy but that's actually all you need to do. If you require any special kernel configs be sure to change those when the makeconfig pops up but for most users you shouldn't. Once that's done you will have 3 files that end in .pkg.tar.xz. You will need to install all 3. First, install the headers, the docs, then the kernel like so. \n\n    pacman -U linux-mainline-headers-3.18.5-1-x86_64.pkg.tar.xz\n    pacman -U linux-mainline-docs-3.18.5-1-x86_64.pkg.tar.xzz\n    pacman -U linux-mainline-3.18.5-1-x86_64.pkg.tar.xz\n\nOnce that's done we'll need to add the pci-stub module to initramfs. Vi or nano /etc/mkinitcpio.conf and change MODULES=\"\" to this MODULES=\"pci-stub\". Next run mkinitcpio to update our initramfs\n\n    mkinitcpio -p linux-mainline\n\nNow that we've updated our initramfs we need to update grub. This part may be the most difficult simply because you will need to do independent research and decide which options are appropriate for you. First, here is my grub config. This file is stored in /etc/default/grub\n\n    GRUB_CMDLINE_LINUX_DEFAULT=\"pcie_acs_override=downstream intel_iommu=on pci-stub.ids=10de:13c2,10de:0fbb\"\n\npcie_acs_override=downstream fixes an issue on my motherboard with IOMMU groups. If you leave this out and KVM gripes about not all devices being accessible then you may need this enabled. A full writeup on the subject can be found [here](http://vfio.blogspot.com/2014/08/iommu-groups-inside-and-out.html). intel_iommu=on ensures that IOMMU is enabled within the kernel. pci-stub.ids=.. is our main concern. This has to be ran before the kernel is loaded so that the GPU is not allocated by the kernel allowing us to allocate it to a VM. Remember us adding pci-stub to initramfs? We will be blacklisting these cards using the device codes shown by lspci. Here is example output\n\n    [archie@arch kernel]$ lspci -nn |grep -i nvidia\n    01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM204 [GeForce GTX 970] [10de:13c2] (rev a1)\n    01:00.1 Audio device [0403]: NVIDIA Corporation GM204 High Definition Audio Controller [10de:0fbb] (rev a1)\n    06:00.0 VGA compatible controller [0300]: NVIDIA Corporation GT200 [GeForce GTX 260] [10de:05e2] (rev a1)\n\nNote that my device codes are 10de:13c2 and 10de:0fbb. 01:00.0 and 01:00.1 are defined by the kernel and respective to the pci bus location. 01 being my first PCI-E slot. So, since we have two device codes that we need to blacklist we use the comma delimiter to differentiate the two like so. **NOTE: THIS WILL BLACKLIST YOUR GPU FROM THE KERNEL**\n\n    pci-stub.ids=10de:13c2,10de:0fbb\n\nNow that we're done with that we need to update grub. It should take care of everything for you and essentially gives you 4 kernels. Linux-mainline, a fallback, and your old kernel with a fallback. \n\n    grub-mkconfig -o /boot/grub/grub.cfg\n\nVerify that your pci-stub and other options are loaded by looking at /boot/grub/grub.cfg. You should see linux /vmlinuz-linux-mainline .. and then your options. If you are satisfied then reboot the machine. Once you have loaded back up run a \"dmesg |grep pci-stub\" and look for something similar to this. The device codes should match what you put in grub. \n\n    [    2.096151] pci-stub: add 1002:6719 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000\n    [    2.096160] pci-stub 0000:07:00.0: claimed by stub\n    [    2.096165] pci-stub: add 1002:AA80 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000\n    [    2.096174] pci-stub 0000:07:00.1: claimed by stub\n    [    2.096178] pci-stub: add 1B21:1042 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000\n\nAssuming all is well we simply need to start our virtual machine. You'll need to install a few components now. You're going to need qemu and ovmf-svn. The latter can be pulled from the AUR [here](https://aur.archlinux.org/packages/ovmf-svn)\n\n    pacman -Syy\n    pacman -S qemu\n    wget https://aur.archlinux.org/packages/ov/ovmf-svn/ovmf-svn.tar.gz\n    tar -xvzf ovmf-svn.tar.gz && cd ovmf-svn\n    makepkg\n    pacman -U ovmf-svn-17048-1-x86_64.pkg.tar.xz\n\nOnce done we should just have to do a little scripting to set everything up. Almost done! We need to bind the gpu with vfio-pci. nbhs kindly provides us with a script to do just that,. Save it under /usr/bin/vfio-bind and make sure to chmod 755 /usr/bin/vfio-bind so we can use it!\n\n    #!/bin/bash\n    \n    modprobe vfio-pci\n    \n    for dev in \"$@\"; do\n            vendor=$(cat /sys/bus/pci/devices/$dev/vendor)\n            device=$(cat /sys/bus/pci/devices/$dev/device)\n            if [ -e /sys/bus/pci/devices/$dev/driver ]; then\n                    echo $dev > /sys/bus/pci/devices/$dev/driver/unbind\n            fi\n            echo $vendor $device > /sys/bus/pci/drivers/vfio-pci/new_id\n    done\n\nTo bind our GPU we simply use this. Remember that my GPU is located at 01.00.0 and 01.00.1 so adjust yours accordingly. Check with lspci -nn\n\n    vfio-bind 0000:01:00.0 0000:01:00.1\n\nNow, you should be able to test using the i440fx but if it doesn't work don't be disheartened. If you see output then fantastic, otherwise keep reading and try the later steps as well. I have a 4790k which means I have 4 cores and 2 threads per core, adjust accordingly. Also be sure to change the device ID in the vfio-pci options. \n\n    qemu-system-x86_64 -enable-kvm -m 1024 -cpu host \\\n    -smp cores=4,threads=2 \\\n    -device vfio-pci,host=01:00.0,x-vga=on \\\n    -device vfio-pci,host=01:00.1 \\\n    -vga none \n\nNow, I never got the i440fx to work. So I'll share my OVMF method. This is the full unadulterated script which I'll break down afterwards. \n\n    #!/bin/bash\n    \n    xrandr --output DVI-I-1 --off\n    QEMU_PA_SAMPLES=128 QEMU_AUDIO_DRV=pa\n    qemu-system-x86_64 -enable-kvm -m 8192 -cpu host,kvm=off -smp cores=4,threads=2 \\\n    -pflash /usr/share/ovmf/x64/ovmf_x64.bin \\\n    -vga none \\\n    -usb -usbdevice host:1b1c:1b09 -usbdevice host:046d:c07d \\\n    -soundhw hda \\\n    -device vfio-pci,host=01:00.0,multifunction=on \\\n    -device vfio-pci,host=01:00.1 \\\n    -device virtio-scsi-pci,id=scsi \\\n    -drive file=/home/tully/VMs/win10.iso,id=isocd,format=raw,if=none -device scsi-cd,drive=isocd \\\n    -drive file=/dev/sdb,id=disk,format=raw,if=none -device scsi-hd,drive=disk \\\n    -drive file=/home/tully/VMs/virt.iso,id=virtiocd,if=none -device ide-cd,bus=ide.1,drive=virtiocd \\\n    -boot menu=on\n    pkill conky\n    fixmonitors\n    conky -d\n\nFirst we kill my left monitor. This puts it into autodetect mode. It sees the 970 outputting video and switches to that input automagically. Next I set some environment variables to get sound working. Everything from qemu-system-x86_64 to -boot menu=on is one command. --enable-kvm tells qemu to use the kvm module, -m 8192 allocates 8192MB of RAM to the guest VM, kvm=off is a built in feature that attempts to hide the fact that any virtual machine stuff is going on. The Nvidia driver will code 43 if it detects a hypervisor. (they want you to buy quadros), the 4970k has 4 cores and 2 threads per core, -plfash tells qemu to use the OVMF bios, -vga none says to not emulate video, -usb steals my keyboard and mouse and passes it through to the VM, -soundhw sets our sound, -device vfio-pci sets our gpu up with qemu, -device virtio-scsi sets up the OVMF devices, first -drive is the windows 10 iso i used to install, second is my velociraptor which is located using fdisk -l, third is the virtio driver used to get windows to work with the OVMF devices and can be found [here](https://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/). Set -boot menu=on so you can change the boot options if needed. Once you load the Windows iso it will complain that it can't find a drive. Just click the load driver button, navigate through the iso to find Windows 7 or 8.1 and select the only option it gives you. Should show your hdd fine after that. \n\nEverything else is just personally needed and runs after the guest VM shuts down. I kill conky because turning the monitor off and on messes up the positioning, fixmonitors is a script I wrote to get my monitors working again with xrandr and is pretty much just dependent on your setup and conky -d just starts conky again. \n\n---------\n\nWow that was a lot of typing. I'm most likely going to be going back over this and revising as needed but it should serve as a good starting point for many looking to do this. Once you have everything set up you won't have to touch it again. It seems like a huge pain in the ass but to get away from the dependence of Windows for my everyday activities is worth far more effort in my opinion. Thanks everyone for sticking with me, please let me know if you see any mistakes or information you'd like added, and I'll be happy to answer any questions you have. \n\nTL;DR - Linux is awesome\n\nEdit: removed mention of the vga arbitration patch as it is not needed with the ovmf bios. ",
  "hide_score": false,
  "_api_link": "https://api.reddit.com/r/linux_gaming/comments/2z2y7h/gpu_passthrough_or_how_to_play_any_game_at_near/?ref=search_posts",
  "locked": false,
  "edited": 1442738366.0,
  "title": "GPU Passthrough - Or How To Play Any Game At Near Native Performance In A Virtual Machine (xpost /r/pcmasterrace)",
  "created": 1426388199.0,
  "_comments_by_id": {}
}